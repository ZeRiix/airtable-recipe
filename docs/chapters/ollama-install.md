# Installation d'Ollama

Ollama est requis pour faire fonctionner l'IA de g√©n√©ration de recettes. Voici comment l'installer selon votre syst√®me d'exploitation.

## üêß Linux

### Installation via le script officiel
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Installation manuelle
1. T√©l√©chargez le binaire depuis [ollama.com](https://ollama.com)
2. Rendez-le ex√©cutable et d√©placez-le dans votre PATH :
```bash
chmod +x ollama
sudo mv ollama /usr/local/bin/
```

## üçé macOS

### Via Homebrew (recommand√©)
```bash
brew install ollama
```

### T√©l√©chargement direct
1. T√©l√©chargez l'application depuis [ollama.com](https://ollama.com)
2. Glissez-d√©posez dans votre dossier Applications
3. Lancez l'application une premi√®re fois

## ü™ü Windows

### Installation via l'installateur
1. T√©l√©chargez l'installateur Windows depuis [ollama.com](https://ollama.com)
2. Ex√©cutez le fichier `.exe` et suivez les instructions
3. Red√©marrez votre terminal apr√®s l'installation

### Via Windows Subsystem for Linux (WSL)
Si vous utilisez WSL, suivez les instructions Linux ci-dessus.

## ‚úÖ V√©rification de l'installation

Apr√®s l'installation, v√©rifiez qu'Ollama fonctionne :

```bash
ollama --version
```

Vous devriez voir la version d'Ollama s'afficher.

## üöÄ Premier lancement

Le projet se charge automatiquement de t√©l√©charger le mod√®le requis (`qwen2.5:3b`) lors du premier lancement avec `npm run dev`.

Si vous souhaitez t√©l√©charger le mod√®le manuellement :

```bash
ollama pull qwen2.5:3b
```

## üîß Configuration

Ollama d√©marre automatiquement avec le projet. Le serveur Ollama √©coute par d√©faut sur `http://localhost:11434`.

Si vous rencontrez des probl√®mes de port, vous pouvez configurer Ollama pour utiliser un port diff√©rent :

```bash
export OLLAMA_HOST=0.0.0.0:11435
ollama serve
```

## üìã Mod√®les support√©s

Le projet utilise actuellement le mod√®le `qwen2.5:3b` qui offre un bon √©quilibre entre performance et qualit√© pour la g√©n√©ration de recettes.

D'autres mod√®les peuvent √™tre utilis√©s en modifiant la configuration dans le code source (voir `services/bff/interfaces/providers/ollama/index.ts`).

## üêõ D√©pannage

### Ollama ne d√©marre pas
- V√©rifiez que le port 11434 n'est pas utilis√© par un autre processus
- Sur Linux/macOS, v√©rifiez les permissions d'ex√©cution
- Red√©marrez votre terminal apr√®s l'installation

### Le mod√®le ne se t√©l√©charge pas
- V√©rifiez votre connexion internet
- Assurez-vous d'avoir suffisamment d'espace disque (le mod√®le fait environ 2.2GB)
- Essayez de t√©l√©charger manuellement : `ollama pull qwen2.5:3b`

### Erreur de connexion
- V√©rifiez qu'Ollama est bien d√©marr√© : `ollama list`
- V√©rifiez l'URL dans la configuration du projet
- Testez manuellement : `curl http://localhost:11434/api/tags`

## üìö Ressources

- [Documentation officielle Ollama](https://ollama.com/docs)
- [Liste des mod√®les disponibles](https://ollama.com/library)
- [Guide de d√©pannage Ollama](https://ollama.com/docs/troubleshooting)
